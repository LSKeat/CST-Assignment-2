sudo apt install openjdk-11-jdk -y (download java)

wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz (download hadoop)

tar -xvzf hadoop-3.3.6.tar.gz (extract hadoop)

sudo mv hadoop-3.3.6 /usr/local/hadoop (move Hadoop to path)

nano ~/.bashrc (set environment variable)

export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

source ~/.bashrc (apply the changes)

cd /usr/local/hadoop/etc/hadoop (go to this path)

export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64  (modify hadoop-env.sh)

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>     (modify core-site.xml)

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///usr/local/hadoop_data/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///usr/local/hadoop_data/hdfs/datanode</value>
    </property>
</configuration>  (modify hdfs-site.xml)

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>

    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>

    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
    </property>
</configuration>  (modify mapred-site.xml)

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>    (modify yarn-site.xml)

sudo mkdir -p /usr/local/hadoop_data/hdfs/namenode
sudo mkdir -p /usr/local/hadoop_data/hdfs/datanode
sudo chown -R $USER:$USER /usr/local/hadoop_data     (Create Hadoop Data Directories)

hdfs namenode -format  (format the NameNode)


Ensure that SSH is installed on your system:
sudo apt update
sudo apt install openssh-server openssh-client -y

Set Up Passwordless SSH for Localhost  (communicate without manual intervention)
Hadoop requires passwordless SSH access for communication between daemons(NameNode, datanode).
Generate an SSH Key: Run the following command to generate a new SSH key:

ssh-keygen -t rsa -P "" 

This will create a key pair (id_rsa and id_rsa.pub) in the ~/.ssh directory.

Add the Public Key to Authorized Keys 

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


Set Correct Permissions: Ensure the ~/.ssh directory and its files have the correct permissions:

chmod 700 ~/.ssh
chmod 600 ~/.ssh/authorized_keys


Test SSH Access: Verifies that passwordless SSH is correctly configured
ssh localhost

Update Hadoop Configuration

nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh  (ensure SSH works smoothly for Hadoop without interruptions)

export HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no"

start-dfs.sh  (Start HDFS)
start-yarn.sh (start-yarn.sh)
jps (Check running processes)

Firewall setting

Access Web Interfaces:
HDFS NameNode: http://<vm-external-ip>:9870.
YARN ResourceManager: http://<vm-external-ip>:8088.

Our mapreduce will do the task for determine the highest temperature in each year
Prepare input data
2021    35
2021    30
2020    28
2021    32
2019    25
2019    30
2018    27
2018    29

hadoop fs -mkdir /input
hadoop fs -put data.txt /input  (Upload the file to HDFS)

Mapper.py
#!/usr/bin/env python3
import sys

# Read each line of input
for line in sys.stdin:
    line = line.strip()
    try:
        tokens = line.split()
        if len(tokens) >= 2:
            year = tokens[0]  # First token is the year
            temperature = int(tokens[1].strip())  # Second token is the temperature

            # Emit key-value pair: year as key, temperature as value
            print(f"{year}\t{temperature}")
    except ValueError:
        pass  # Ignore lines with invalid temperature

Reducer
#!/usr/bin/env python3
import sys

# Initialize variables for the current year and the highest temperature
current_year = None
max_temperature = None

# Read input line by line
for line in sys.stdin:
    line = line.strip()
    
    # Parse the input (year, temperature)
    year, temperature = line.split('\t')
    temperature = int(temperature)
    
    # If the year has changed, print the max temperature for the previous year
    if current_year and current_year != year:
        print(f"{current_year}\t{max_temperature}")
        max_temperature = temperature
    
    # Update the max temperature for the current year
    if current_year != year:
        current_year = year
        max_temperature = temperature
    else:
        max_temperature = max(max_temperature, temperature)

# Output the last year's maximum temperature
if current_year:
    print(f"{current_year}\t{max_temperature}")

hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \  
  -input /input/data.txt \
  -output /output \
  -mapper "python3 mapper.py" \
  -reducer "python3 reducer.py" \
  -file mapper.py \
  -file reducer.py

hadoop fs -cat /output/part-r-00000

